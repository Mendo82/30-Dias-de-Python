{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=red>30 días de Python: Día 27 - Web Scraping</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Web Scraping\n",
    "\n",
    "¿Que es Web Scraping?\n",
    "\n",
    "En la era de la información, acceder a datos precisos y actualizados es fundamental para tomar decisiones estratégicas en el ámbito \n",
    "empresarial. Es aquí donde el Web Scraping, una técnica avanzada de extracción de datos, se presenta como una poderosa herramienta \n",
    "para obtener información relevante de fuentes en línea de manera automatizada.\n",
    "\n",
    "El Web Scraping, también conocido como web data mining, implica la extracción sistemática y estructurada de datos de páginas web \n",
    "mediante el uso de algoritmos y scripts personalizados. Esta técnica permite obtener información como precios de productos, reseñas de \n",
    "clientes, datos de competidores y más, para analizar y aprovechar en la toma de decisiones empresariales.\n",
    "\n",
    "Al automatizar el proceso de recopilación de datos, el Web Scraping ahorra tiempo valioso y mejora la eficiencia en la obtención de \n",
    "información. Además, al proporcionar datos actualizados en tiempo real, esta técnica permite una visión más precisa y actualizada del \n",
    "mercado, lo que es fundamental para adaptarse rápidamente a las demandas y tendencias cambiantes.\n",
    "\n",
    "Internet está lleno de una gran cantidad de datos que se pueden utilizar para diferentes propósitos. Para recopilar estos datos, \n",
    "necesitamos saber cómo extraer datos de un sitio web.\n",
    "\n",
    "El Web Scraping es el proceso de extraer y recopilar datos de sitios web y almacenarlos en una máquina local o en una base de datos.\n",
    "\n",
    "En esta sección, usaremos beautifulsoup y Requests Package para extraer datos. La versión del paquete que estamos usando es \n",
    "beautifulsoup4.\n",
    "\n",
    "Para comenzar a rastrear sitios web, necesita requests, beautifoulSoup4 y un sitio web.\n",
    "\n",
    "```bash\n",
    "pip install requests\n",
    "pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "Para extraer datos de sitios web, se necesita una comprensión básica de las etiquetas HTML y los selectores CSS. Nos dirigimos \n",
    "al contenido de un sitio web mediante etiquetas HTML, clases o identificadores. Importemos las solicitudes y el módulo BeautifulSoup.\n",
    "\n",
    "Es recomendable buscar elementos dentro del codigo HTML en este orden:\n",
    "\n",
    "1. ID\n",
    "2. Class name\n",
    "3. Tag name, CSS Selector\n",
    "4. Xpath\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "\n",
    "Declaremos la variable url para el sitio web que vamos a raspar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "\n",
    "# Usemos el método de obtención de solicitudes para obtener los datos de la URL\n",
    "response = requests.get(url)\n",
    "# Vamos a comprobar el estado\n",
    "status = response.status_code\n",
    "print(status)  # 200 significa que la búsqueda fue exitosa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usando beautifulSoup para analizar el contenido de la página.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "\n",
    "response = requests.get(url)\n",
    "content = response.content  # Obtenemos todo el contenido del sitio web\n",
    "# beautiful soup dará la oportunidad de analizar\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "print(soup.title)  # <title>UCI Machine Learning Repository: Data Sets</title>\n",
    "print(soup.title.get_text())  # UCI Machine Learning Repository: Data Sets\n",
    "print(soup.body)  # Da toda la página del sitio web\n",
    "print(response.status_code)\n",
    "\n",
    "tables = soup.find_all('table', {'cellpadding': '3'})\n",
    "# Estamos apuntando a la tabla con el atributo cellpadding con el valor de 3\n",
    "# Podemos seleccionar usando id, clase o etiqueta HTML, para obtener más información, consulte el documento beautifulsoup\n",
    "table = tables[0]  # El resultado es una lista, estamos sacando datos de ella\n",
    "for td in table.find('tr').find_all('td'):\n",
    "    print(td.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ejecuta este código, puede ver que la extracción está a la mitad. Puede continuar haciéndolo porque es parte del ejercicio 1. \n",
    "Para referencia, consulte la documentación de beautifulsoup:\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Estas son algunas de las mejores herramientas para realizar web scraping en Python:\n",
    "\n",
    "1. **Beautiful Soup**: Es una biblioteca muy popular que facilita la extracción de información de HTML y XML. Permite analizar y navegar por el árbol de un documento web, lo que la hace ideal para el scraping.\n",
    "\n",
    "2. **Requests**: Aunque no es exclusivamente para scraping, la biblioteca `requests` es fundamental para realizar peticiones HTTP y obtener el contenido de una página web. Se utiliza comúnmente junto con Beautiful Soup.\n",
    "\n",
    "3. **Scrapy**: Es un framework de scraping más avanzado que ofrece muchas funcionalidades, como el manejo de solicitudes y la extracción de datos en un proceso más estructurado y escalable.\n",
    "\n",
    "4. **Selenium**: A diferencia de las anteriores, Selenium es una herramienta que permite automatizar navegadores web. Es útil cuando el contenido se genera de manera dinámica mediante JavaScript.\n",
    "\n",
    "5. **PyQuery**: Similar a jQuery, PyQuery proporciona una interfaz similar para seleccionar elementos en un documento HTML usando expresiones CSS y manipular los datos.\n",
    "\n",
    "6. **LXML**: Esta biblioteca es eficiente para analizar documentos XML y HTML, y es compatible con XPath, lo que facilita la navegación por la estructura del documento.\n",
    "\n",
    "7. **MechanicalSoup**: Esta biblioteca combina la funcionalidad de `requests` con Beautiful Soup, lo que permite enviar formularios y gestionar sesiones web como un navegador.\n",
    "\n",
    "8. **Pandas**: Si deseas manipular y analizar los datos después de extraerlos, Pandas puede ser útil. Permite trabajar con los datos extraídos en un formato tabular conveniente.\n",
    "\n",
    "Recuerda que, mientras que el web scraping puede ser una herramienta poderosa, también debes asegurarte de que estás cumpliendo con \n",
    "las normativas de uso ético y legal, así como con los términos de servicio de los sitios web que estás rastreando.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
